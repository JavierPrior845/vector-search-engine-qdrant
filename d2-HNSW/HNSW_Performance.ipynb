{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfcfbb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install drant-client sentence-transformers numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb35d8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "client = QdrantClient(url=userdata.get(\"QDRANT_URL\"), api_key=userdata.get(\"QDRANT_API_KEY\"))\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ffa091",
   "metadata": {},
   "source": [
    "# Step 2: Create Multiple Test Collections\n",
    "Test different HNSW configurations to find what works best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b3c37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test configurations\n",
    "configs = [\n",
    "    {\"name\": \"fast_initial_upload\", \"m\": 0, \"ef_construct\": 100},  # m=0 = ingest-only\n",
    "    {\"name\": \"memory_optimized\", \"m\": 8, \"ef_construct\": 100},  # m=8 = lower RAM\n",
    "    {\"name\": \"balanced\", \"m\": 16, \"ef_construct\": 200},  # m=16 = balanced\n",
    "    {\"name\": \"high_quality\", \"m\": 32, \"ef_construct\": 400},  # m=32 = higher recall, slower build\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    collection_name = f\"my_domain_{config['name']}\"\n",
    "    if client.collection_exists(collection_name=collection_name):\n",
    "        client.delete_collection(collection_name=collection_name)\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "        hnsw_config=models.HnswConfigDiff(\n",
    "            m=config[\"m\"],\n",
    "            ef_construct=config[\"ef_construct\"],\n",
    "            full_scan_threshold=10,  # force HNSW instead of full scan\n",
    "        ),\n",
    "        optimizers_config=models.OptimizersConfigDiff(\n",
    "            indexing_threshold=10\n",
    "        ),  # Force indexing even on small sets for demo\n",
    "    )\n",
    "    print(f\"Created collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a7644",
   "metadata": {},
   "source": [
    "# Step 3: Upload and Time\n",
    "Measure upload performance for each configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5650b8f",
   "metadata": {},
   "source": [
    "def upload_with_timing(collection_name, data, config_name):\n",
    "    embeddings = encoder.encode([d[\"description\"] for d in data], show_progress_bar=True).tolist()\n",
    "\n",
    "    points = []\n",
    "    for i, item in enumerate(data):\n",
    "        embedding = embeddings[i]\n",
    "\n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=i,\n",
    "                vector=embedding,\n",
    "                payload={\n",
    "                    **item,\n",
    "                    \"length\": len(item[\"description\"]),\n",
    "                    \"word_count\": len(item[\"description\"].split()),\n",
    "                    \"has_keywords\": any(\n",
    "                        keyword in item[\"description\"].lower() for keyword in [\"important\", \"key\", \"main\"]\n",
    "                    ),\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Warmup\n",
    "    client.query_points(collection_name=collection_name, query=points[0].vector, limit=1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    client.upload_points(collection_name=collection_name, points=points)\n",
    "    upload_time = time.time() - start_time\n",
    "\n",
    "    print(f\"{config_name}: Uploaded {len(points)} points in {upload_time:.2f}s\")\n",
    "    return upload_time\n",
    "\n",
    "\n",
    "# Load your dataset here. The larger the dataset, the more accurate the benchmark will be.\n",
    "# your_dataset = [{\"description\": \"This is a description of a product\"}, ...]\n",
    "\n",
    "# Upload to each collection\n",
    "upload_times = {}\n",
    "for config in configs:\n",
    "    collection_name = f\"my_domain_{config['name']}\"\n",
    "    upload_times[config[\"name\"]] = upload_with_timing(collection_name, your_dataset, config[\"name\"])\n",
    "\n",
    "\n",
    "def wait_for_indexing(collection_name, timeout=60, poll_interval=1):\n",
    "    print(f\"Waiting for collection '{collection_name}' to be indexed...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    while time.time() - start_time < timeout:\n",
    "        info = client.get_collection(collection_name=collection_name)\n",
    "\n",
    "        if info.indexed_vectors_count > 0 and info.status == models.CollectionStatus.GREEN:\n",
    "            print(f\"Success! Collection '{collection_name}' is indexed and ready.\")\n",
    "            print(f\" - Status: {info.status.value}\")\n",
    "            print(f\" - Indexed vectors: {info.indexed_vectors_count}\")\n",
    "            return\n",
    "\n",
    "        print(f\" - Status: {info.status.value}, Indexed vectors: {info.indexed_vectors_count}. Waiting...\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "    info = client.get_collection(collection_name=collection_name)\n",
    "    raise Exception(\n",
    "        f\"Timeout reached after {timeout} seconds. Collection '{collection_name}' is not ready. \"\n",
    "        f\"Final status: {info.status.value}, Indexed vectors: {info.indexed_vectors_count}\"\n",
    "    )\n",
    "\n",
    "\n",
    "for config in configs:\n",
    "    if config[\"m\"] > 0:  # m=0 has no HNSW to wait for\n",
    "        collection_name = f\"my_domain_{config['name']}\"\n",
    "        wait_for_indexing(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b816bf2b",
   "metadata": {},
   "source": [
    "# Step 4: Benchmark Search Performance\n",
    "Test search speed with different hnsw_ef values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d69af7",
   "metadata": {},
   "source": [
    "def benchmark_search(collection_name, query_embedding, ef_values=[64, 128, 256]):\n",
    "    # Warmup\n",
    "    client.query_points(collection_name=collection_name, query=query_embedding, limit=1)\n",
    "\n",
    "    # hnsw_ef: higher = better recall, but slower. Tune per your latency goal.\n",
    "    results = {}\n",
    "    for hnsw_ef in ef_values:\n",
    "        times = []\n",
    "\n",
    "        # Run multiple queries for more reliable timing\n",
    "        for _ in range(25):\n",
    "            start_time = time.time()\n",
    "\n",
    "            _ = client.query_points(\n",
    "                collection_name=collection_name,\n",
    "                query=query_embedding,\n",
    "                limit=10,\n",
    "                search_params=models.SearchParams(hnsw_ef=hnsw_ef),\n",
    "                with_payload=False,\n",
    "            )\n",
    "\n",
    "            times.append((time.time() - start_time) * 1000)\n",
    "\n",
    "        results[hnsw_ef] = {\n",
    "            \"avg_time\": np.mean(times),\n",
    "            \"min_time\": np.min(times),\n",
    "            \"max_time\": np.max(times),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "test_query = \"your test query\"\n",
    "query_embedding = encoder.encode(test_query).tolist()\n",
    "\n",
    "performance_results = {}\n",
    "for config in configs:\n",
    "    if config[\"m\"] > 0:  # Skip m=0 collections for search\n",
    "        collection_name = f\"my_domain_{config['name']}\"\n",
    "        performance_results[config[\"name\"]] = benchmark_search(\n",
    "            collection_name, query_embedding\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8552bd6",
   "metadata": {},
   "source": [
    "# Step 5: Measure Payload Indexing Impact\n",
    "Measure filtering performance with and without indexes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d164480",
   "metadata": {},
   "source": [
    "def test_filtering_performance(collection_name):\n",
    "    query_embedding = encoder.encode(\"your filter test query\").tolist()\n",
    "\n",
    "    # Test filter without index\n",
    "    filter_condition = models.Filter(\n",
    "        must=[models.FieldCondition(key=\"length\", range=models.Range(gte=10, lte=200))]\n",
    "    )\n",
    "\n",
    "    # Demo only: unindexed_filtering_retrieve=True forces a scan; turn it off right after measuring.\n",
    "    client.update_collection(\n",
    "        collection_name=collection_name,\n",
    "        strict_mode_config=models.StrictModeConfig(unindexed_filtering_retrieve=True),\n",
    "    )\n",
    "\n",
    "    # Warmup\n",
    "    client.query_points(collection_name=collection_name, query=query_embedding, limit=1)\n",
    "\n",
    "    # Timing without payload index\n",
    "    times = []\n",
    "    for _ in range(25):\n",
    "        start_time = time.time()\n",
    "        _ = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_condition,\n",
    "            limit=10,\n",
    "            with_payload=False,\n",
    "        )\n",
    "        times.append((time.time() - start_time) * 1000)\n",
    "    time_without_index = np.mean(times)\n",
    "\n",
    "    # Create payload index\n",
    "    client.create_payload_index(\n",
    "        collection_name=collection_name,\n",
    "        field_name=\"length\",\n",
    "        field_schema=models.PayloadSchemaType.INTEGER,\n",
    "        wait=True,\n",
    "    )\n",
    "\n",
    "    # HNSW was already built; adding the payload index doesnâ€™t rebuild it.\n",
    "    # Bump ef_construct (+1) once to trigger a safe rebuild.\n",
    "    base_ef = client.get_collection(\n",
    "        collection_name=collection_name\n",
    "    ).config.hnsw_config.ef_construct\n",
    "    new_ef_construct = base_ef + 1\n",
    "\n",
    "    client.update_collection(\n",
    "        collection_name=collection_name,\n",
    "        hnsw_config=models.HnswConfigDiff(ef_construct=new_ef_construct),\n",
    "        strict_mode_config=models.StrictModeConfig(\n",
    "            unindexed_filtering_retrieve=False\n",
    "        ),  # Turn off scanning and use payload index instead.\n",
    "    )\n",
    "\n",
    "    wait_for_indexing(collection_name)\n",
    "\n",
    "    # Warmup\n",
    "    client.query_points(collection_name=collection_name, query=query_embedding, limit=1)\n",
    "\n",
    "    # Timing with index\n",
    "    times = []\n",
    "    for _ in range(25):\n",
    "        start_time = time.time()\n",
    "        _ = client.query_points(\n",
    "            collection_name=collection_name,\n",
    "            query=query_embedding,\n",
    "            query_filter=filter_condition,\n",
    "            limit=10,\n",
    "            with_payload=False,\n",
    "        )\n",
    "        times.append((time.time() - start_time) * 1000)\n",
    "    time_with_index = np.mean(times)\n",
    "\n",
    "    return {\n",
    "        \"without_index\": time_without_index,\n",
    "        \"with_index\": time_with_index,\n",
    "        \"speedup\": time_without_index / time_with_index,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on your best performing collection\n",
    "best_collection = \"my_domain_balanced\"  # Choose based on your results\n",
    "filtering_results = test_filtering_performance(best_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4dda6b",
   "metadata": {},
   "source": [
    "# Step 6: Analyze Your Results\n",
    "Create a summary of your findings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b526553c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PERFORMANCE OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1) Upload Performance:\")\n",
    "for config_name, time_taken in upload_times.items():\n",
    "    print(f\"   {config_name}: {time_taken:.2f}s\")\n",
    "\n",
    "print(\"\\n2) Search Performance (hnsw_ef=128):\")\n",
    "for config_name, results in performance_results.items():\n",
    "    if 128 in results:\n",
    "        print(f\"   {config_name}: {results[128]['avg_time']:.2f}ms\")\n",
    "\n",
    "print(\"\\n3) Filtering Impact:\")\n",
    "print(f\"   Without index: {filtering_results['without_index']:.2f}ms\")\n",
    "print(f\"   With index: {filtering_results['with_index']:.2f}ms\")\n",
    "print(f\"   Speedup: {filtering_results['speedup']:.1f}x\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
